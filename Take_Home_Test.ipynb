{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_jcCK5ncrsw"
   },
   "source": [
    "# Convergence ML Engineer / Researcher Take-home\n",
    "\n",
    "We'd like to learn a little more about how you practically approach a small research-like project loosely based on Rejection Sampling Fine-tuning (aka RFT, introduced in https://arxiv.org/abs/2308.01825).\n",
    "\n",
    "Tip: focus on section 3.3 (\"Rejection Sampling Fine-tuning\"). The paper isn't the best written, and we're happy to clarify anything.\n",
    "\n",
    "We will provide some skeleton code for you to guide what we would like to see from you, although if you have ideas for a different structure you feel is better or more elegant, then feel free to rewrite and replace at will.\n",
    "\n",
    "Note: your final submission does not have to be in a colab notebook, does not have to use Hugging Face, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## A note from the team\n",
    "\n",
    "We want to give you a chance to show off some of your best abilities.\n",
    "\n",
    "For some people that might mean generating high quality data in a smart way. For others, it might be speeding up the whole process to enable easy reproducibility, and maybe organizing the code in a better way than given. Yet for others, it might be a chance to show off some modern policy optimization techniques like DPO or its variants. Or maybe focusing on solid evaluations and identifying limitations of small models and limited fine-tuning.\n",
    "\n",
    "An ideal outcome of course is some sense of the model improving its mathematical abilities, but itâ€™s not a bad thing if the final evaluation somehow shows equal or worse performance ðŸ˜‚ (negative results are results).\n",
    "\n",
    "Ask lots of question! We're happy to answer any questions about the assignment, and to discuss concepts like RFT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojrmHoW2dLdU"
   },
   "source": [
    "# Setup [ignore - just run]\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "zPfAkbvHdEwJ"
   },
   "outputs": [],
   "source": [
    "#!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os  \n",
    "import torch\n",
    "import random \n",
    "import datasets\n",
    "import numpy as np \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from data import GSM8KDataset, _apply_template\n",
    "from prompt import EvalTemplate\n",
    "\n",
    "import utils\n",
    "import generation \n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "SEED = 128 \n",
    "MODEL_NAME = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "# set seeds\n",
    "torch.random.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61818c5e77224e57b34d5d354940c3f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Training instances: 7473\n",
      "Num Validation instances: 1319\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16, #Â accelerate inf \n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_dataset = datasets.load_dataset('gsm8k', 'main')['train']\n",
    "val_dataset = datasets.load_dataset('gsm8k', 'main')['test'] \n",
    "print(f\"Num Training instances: {len(train_dataset)}\")\n",
    "print(f\"Num Validation instances: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 7473\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Checking instance 3282:\n",
      "question\n",
      "Itâ€™s February 2021.  Mark was born in January 1976.  Graham is 3 years younger than Mark, and Grahamâ€™s sister, Janice, is 1/2 the age of Graham.  How old is Janice?\n",
      "answer\n",
      "Itâ€™s 2021 and Mark was born in 1976 so Mark is 2021-1976 = <<2021-1976=45>>45 years old\n",
      "Graham is 3 years younger than Mark who is 45 so Graham is 45-3 = 42 years old\n",
      "Janice is 1/2 the age of Graham who is 42 so Janice is 42/2 = <<42/2=21>>21 years old\n",
      "#### 21\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Checking instance 7251:\n",
      "question\n",
      "Melissa sells a coupe for $30,000 and an SUV for twice as much. If her commission is 2%, how much money did she make from these sales?\n",
      "answer\n",
      "First find the total cost of the SUV: $30,000 * 2 = $<<30000*2=60000>>60,000\n",
      "Then add the cost of the coupe to find the total cost of the cars: $60,000 + $30,000 = $<<60000+30000=90000>>90,000\n",
      "Multiplying that amount by Melissa's commission rate, we find her earnings are $90,000 * 2% = $<<90000*2*.01=1800>>1800\n",
      "#### 1800\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Checking instance 5605:\n",
      "question\n",
      "Two years ago, Jared was twice as old as Tom. If Tom will be 30 in five years, how old is Jared now?\n",
      "answer\n",
      "Tom will be 30 in 5 years so now he is 30-5 = <<30-5=25>>25\n",
      "2 years ago Tom was 25-2 = <<25-2=23>>23\n",
      "At that time, Jared was twice as old as Tom so Jared was 23*2 = 46\n",
      "Two years have since passed so Jared is now 46+2 = <<46+2=48>>48\n",
      "#### 48\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Checking instance 2386:\n",
      "question\n",
      "An animal shelter recently took in twelve cats. The shelter already had half that number of cats. Two days later, three had been adopted. The next day, one of the cats had five kittens, and one person arrived to pick up one of the cats that was their missing pet. How many cats does the shelter have now?\n",
      "answer\n",
      "The shelter started with 12 / 2 = <<12/2=6>>6 cats.\n",
      "The cats they took in increased the number to 12 + 6 = <<12+6=18>>18 cats.\n",
      "Three cats were adopted, so they had 18 - 3 = <<18-3=15>>15 cats.\n",
      "One cat had 5 kittens, which increased the number of cats to 15 + 5 = 20 cats.\n",
      "The pet owner came to get his missing cat, so they now have 20 - 1 = <<20-1=19>>19 cats.\n",
      "#### 19\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Checking instance 7287:\n",
      "question\n",
      "Lena has 16 candy bars. She needs 5 more candy bars to have 3 times as many as Kevin, and Kevin has 4 candy bars less than Nicole. How many more candy bars does Lena have than Nicole?\n",
      "answer\n",
      "Kevin has (16 + 5)/3 = <<(16+5)/3=7>>7 candy bars.\n",
      "Nicole has 7 + 4 = <<7+4=11>>11 candy bars.\n",
      "Lena has 16 - 11 = <<16-11=5>>5 more candy bars than Nicole\n",
      "#### 5\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    seed = np.random.randint(0, len(train_dataset))\n",
    "    print(\"*\"*100)\n",
    "    print(f\"Checking instance {seed}:\")\n",
    "    utils.inspect_instance(train_dataset, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only look at train set now for certain information that will be used\n",
    "during inference \n",
    "\n",
    "- Maximum length (num_tokens) of question: 239\n",
    "- Maximum length (num_tokens) of answer: 475 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum answer num_tokens: 475\n",
      "Maximum question num_tokens: 239\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    lambda x: utils.GSM8KParser.get_question_length(x['question'], tokenizer)\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x: utils.GSM8KParser.get_answer_length(x['answer'], tokenizer) \n",
    ")\n",
    "print(f\"Maximum answer num_tokens: {max(train_dataset['answer_length'])}\")\n",
    "print(f\"Maximum question num_tokens: {max(train_dataset['question_length'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â infer number of hops \n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x: utils.GSM8KParser.get_num_hops(x['answer'])\n",
    ")\n",
    "\n",
    "# infer answes using ground truth parser \n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x: utils.GSM8KParser.get_answer_from_gt(x['answer'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optinal Cell (Only to verify that parsing from \n",
    "# ground truth and parsing from completion would \n",
    "# yield the same result \n",
    "# infer answers using prediction parser\n",
    "answer_str_inf = [\n",
    "    utils.GSM8KParser.get_answer_from_pred(x)['answer_str_digit'] \\\n",
    "    for x in train_dataset['answer']\n",
    "]\n",
    "assert answer_str_inf == train_dataset['answer_str_digit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate Dataset \n",
    "- Instead of collate a dataset using collate_fn on the fly. I went for pre-tokenzing the dataset into a static format, avoiding the need to tokenzie data on the fly. This would lead to faster training runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum answer num_tokens: 477\n",
      "Maximum question num_tokens: 334\n",
      "Maximum sequence num_tokens: 811\n",
      "Maximum new tokens in generation: 1024\n",
      "Setup Completed dataset:\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'num_hops', 'answer_str_digit', 'formatted_question', 'formatted_answer', 'question_length', 'answer_length', 'question_input_ids', 'question_attention_mask', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 7473\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.load_dataset('gsm8k', 'main')['train']\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x: utils.GSM8KParser.get_num_hops(x['answer'])\n",
    ")\n",
    "TrainData = GSM8KDataset(train_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum answer num_tokens: 430\n",
      "Maximum question num_tokens: 310\n",
      "Maximum sequence num_tokens: 740\n",
      "Maximum new tokens in generation: 1024\n",
      "Setup Completed dataset:\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'num_hops', 'answer_str_digit', 'formatted_question', 'formatted_answer', 'question_length', 'answer_length', 'question_input_ids', 'question_attention_mask', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1319\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "val_dataset = datasets.load_dataset('gsm8k', 'main')['test'] \n",
    "val_dataset = val_dataset.map(\n",
    "    lambda x: utils.GSM8KParser.get_num_hops(x['answer'])\n",
    ")\n",
    "valData = GSM8KDataset(val_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation \n",
    "\n",
    "* We wanted to validate that our mannual padding is done correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question_input_ids', 'question_attention_mask', 'input_ids', 'attention_mask', 'labels', 'question', 'answer', 'num_hops', 'answer_str_digit', 'formatted_question', 'formatted_answer', 'question_length', 'answer_length'])\n"
     ]
    }
   ],
   "source": [
    "dummy_dataloader = DataLoader(TrainData, batch_size=2, shuffle=False)\n",
    "for batch in dummy_dataloader:\n",
    "    print(batch.keys())\n",
    "    assert batch['input_ids'].shape[0] == 2\n",
    "    assert (batch['input_ids'] == TrainData[:2]['input_ids']).all()\n",
    "    break \n",
    "# first we validated that it's loading a homugenous batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|> You are a highly intelligent assistant who is exceptional at solving Math Problems.<|end|><|user|> *Task*    \n",
      "Think step by step to solve the following question:\n",
      "```question\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "```\n",
      "\n",
      "*Format*\n",
      "1. Write all equations in a single line wihtout breaks in the middle.\n",
      "2. End generation with pattern \"#### <DIGITS>\". Replace <DIGITS> with your final answer<|end|><|assistant|> Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "#### 72<|end|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "instance = TrainData[0]\n",
    "#Â check attention mask for the entire sequence\n",
    "print(tokenizer.decode(\n",
    "    instance[\"input_ids\"][instance['attention_mask']!=0]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "#### 72<|end|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# check labels for the entire sequence\n",
    "print(tokenizer.decode(\n",
    "    instance[\"input_ids\"][instance['labels']!=-100]\n",
    "))\n",
    "# noticed how loss is not measured on <|assistant|>, it belongs to part of the question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|> You are a highly intelligent assistant who is exceptional at solving Math Problems.<|end|><|user|> *Task*    \n",
      "Think step by step to solve the following question:\n",
      "```question\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "```\n",
      "\n",
      "*Format*\n",
      "1. Write all equations in a single line wihtout breaks in the middle.\n",
      "2. End generation with pattern \"#### <DIGITS>\". Replace <DIGITS> with your final answer<|end|><|assistant|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(\n",
    "    instance[\"question_input_ids\"][instance[\"question_attention_mask\"]!=0]\n",
    "))\n",
    "#Â noticed how the question_input_ids only include everything up to <|assistant>| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Vibe Check \n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): Phi3ForCausalLM(\n",
       "    (model): Phi3Model(\n",
       "      (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "      (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x Phi3DecoderLayer(\n",
       "          (self_attn): Phi3FlashAttention2(\n",
       "            (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "            (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "            (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Phi3MLP(\n",
       "            (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "            (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "            (activation_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Phi3RMSNorm()\n",
       "          (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (post_attention_layernorm): Phi3RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): Phi3RMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config = {\n",
    "    \"max_new_tokens\" : TrainData.inf_seq_length,\n",
    "    \"temperature\": 0.1,\n",
    "    \"num_return_sequences\":1,\n",
    "    \"top_p\": 0.9,\n",
    "    \"eos_token_id\":tokenizer.eos_token_id,  # Specify the EOS token\n",
    "    \"pad_token_id\":tokenizer.eos_token_id, \n",
    "    \"do_sample\":True,\n",
    "    \"output_scores\":False,\n",
    "    \"return_dict_in_generate\":True,\n",
    "}\n",
    "model = torch.compile(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate instances \n",
    "* Test the \"easiest\" and the \"hardest\" instances \n",
    "* Noticed that our `utils.GSM8KParser.get_answer_from_pred` works even the model keeps generate after `#### digit`\n",
    "* Noticed that the equation parser `GSM8KParser.parse_equations_from_pred` is not perfectly working, but it has extracted numbers from the text is based on the location of the equal signs inside the equaiton  \n",
    "\n",
    "* Last, I highly suspect that GSM8K has been leaked into Phi-3.5 training, the maj@1 is extermely high in one of my previous experiment, ref to [wandb board](https://api.wandb.ai/links/moed/5dxnwaau) here. The plot shows the batch-wise maj@1 for zero-shot generation by using the EvalTemplate (Though I was using a very complete CoT prompt). Take it as a pintch of salt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natalia sold clips to 48 friends in April and half as many in May, so the equation representing the total number of clips sold in April and May is: 48 + (48/2)\n",
      "\n",
      "Calculating the total: 48 + 24 = 72\n",
      "\n",
      "#### 72\n"
     ]
    }
   ],
   "source": [
    "## question that would need to be solved by the shortest hope \n",
    "sorted_data = sorted(TrainData, key=lambda x: x[\"num_hops\"])\n",
    "instance = sorted_data[0]\n",
    "\n",
    "chats = [instance[\"formatted_question\"]]\n",
    "responses = utils.sample_answers(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    chats,\n",
    "    **generation_config,\n",
    ")\n",
    "print(responses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction -> 72\n",
      "Label -> 72\n"
     ]
    }
   ],
   "source": [
    "answer = instance[\"answer_str_digit\"] \n",
    "prediction =  utils.GSM8KParser.get_answer_from_pred(responses[0])[\"answer_str_digit\"]\n",
    "\n",
    "print(f\"Prediction -> {prediction}\")\n",
    "print(f\"Label -> {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['48+24=72']\n"
     ]
    }
   ],
   "source": [
    "parsed_eqs = utils.GSM8KParser.parse_equations_from_pred(responses[0])\n",
    "print(parsed_eqs[\"equations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve this problem, we need to calculate the number of male and female students, determine how many of each like to play basketball, and then find out how many do not like to play basketball.\n",
      "\n",
      "Step 1: Calculate the number of male and female students.\n",
      "The ratio of male to female students is 3:2, and there are 1000 students in total. To find out how many are male and how many are female, we can set up a proportion:\n",
      "\n",
      "3 males / 2 females = 1000 students / (3 males + 2 females)\n",
      "\n",
      "Let's denote the number of male students as M and the number of female students as F:\n",
      "\n",
      "3M / 2F = 1000 / (M + F)\n",
      "\n",
      "Since M + F = 1000, we can substitute:\n",
      "\n",
      "3M / 2F = 1000 / 1000\n",
      "3M / 2F = 1\n",
      "3M = 2F\n",
      "\n",
      "Now we can solve for M and F:\n",
      "\n",
      "M = (2/3)F\n",
      "\n",
      "Using the total number of students:\n",
      "\n",
      "M + F = 1000\n",
      "(2/3)F + F = 1000\n",
      "(2/3)F + (3/3)F = 1000\n",
      "(5/3)F = 1000\n",
      "F = (3/5) * 1000\n",
      "F = 600\n",
      "\n",
      "Now we can find M:\n",
      "\n",
      "M = (2/3) * 600\n",
      "M = 400\n",
      "\n",
      "Step 2: Calculate the number of students who like to play basketball.\n",
      "2/3 of the male students like to play basketball:\n",
      "\n",
      "(2/3) * 400 = 266.67 (approximately 267 male students)\n",
      "\n",
      "1/5 of the female students like to play basketball:\n",
      "\n",
      "(1/5) * 600 = 120 female students\n",
      "\n",
      "Step 3: Calculate the total number of students who like to play basketball:\n",
      "\n",
      "267 male students + 120 female students = 387 students\n",
      "\n",
      "Step 4: Calculate the number of students who do not like to play basketball:\n",
      "\n",
      "Total students - Students who like basketball = Students who do not like basketball\n",
      "1000 - 387 = 613 students\n",
      "\n",
      "Step 5: Calculate the percentage of students who do not like to play basketball:\n",
      "\n",
      "(613 / 1000) * 100% = 61.3%\n",
      "\n",
      "Therefore, 61.3% of the student population do not like to play basketball.\n",
      "\n",
      "The final answer is \\boxed{61.3\\%}.\n",
      "\n",
      "The answer is: 61.3\\%. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# longest answer \n",
    "instance = sorted_data[-1]\n",
    "\n",
    "chats = [instance[\"question\"]]\n",
    "responses = utils.sample_answers(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    chats,\n",
    "    **generation_config,\n",
    ")\n",
    "print(responses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction -> <INVALID_ANSWER>\n",
      "Label -> 52\n"
     ]
    }
   ],
   "source": [
    "answer = instance[\"answer_str_digit\"] \n",
    "prediction =  utils.GSM8KParser.get_answer_from_pred(responses[0])[\"answer_str_digit\"]\n",
    "\n",
    "print(f\"Prediction -> {prediction}\")\n",
    "print(f\"Label -> {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['267+120=387', '2/3+3/3=1000', '613/1000*100=613', '3/2=1000', '1/5*600=120', '3/2=1000/3+2', '2/3+=1000', '3=2', '3/2=1', '3/2=1000/1000', '5/3=1000', '2/3*400=26667267', '1000-387=613']\n"
     ]
    }
   ],
   "source": [
    "parsed_eqs = utils.GSM8KParser.parse_equations_from_pred(responses[0])\n",
    "# the parsing of the equaitons is not perfect, but it shows the direction which we are going\n",
    "print(parsed_eqs[\"equations\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data \n",
    "***\n",
    "- Here we generate 10 response per question using our base model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Walthrough "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we adjust the generation config \n",
    "# [1] bump up the temperature to increase the uncertainty, this will boost the diversity of the reasoning path\n",
    "# [2] since we are using top-p sampling, by increasing the temperature, we are potentially increasing the \n",
    "# number of tokens inside this top-p pool, which further diversifies the choices of tokens available for us in \n",
    "# each timestamp. We decrease top-p to 0 to further encourage diversify. \n",
    "#Â This is the same as suggested in the scaling paper \n",
    "\n",
    "num_sequence = 10\n",
    "generation_config = {\n",
    "    \"max_new_tokens\" : TrainData.inf_seq_length,\n",
    "    \"temperature\": 0.7,\n",
    "    \"num_return_sequences\":num_sequence,\n",
    "    #\"top_p\": 0.5,\n",
    "    \"eos_token_id\":tokenizer.eos_token_id,  # Specify the EOS token\n",
    "    \"pad_token_id\":tokenizer.eos_token_id, \n",
    "    \"do_sample\":True,\n",
    "    \"output_scores\":False,\n",
    "    \"return_dict_in_generate\":True,\n",
    "}\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3737 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3737 [00:09<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "generations = generation.get_generations(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    TrainData,\n",
    "    batch_size=batch_size,\n",
    "    **generation_config\n",
    ")\n",
    "# force only one generation with a break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 torch.Size([10, 480])\n"
     ]
    }
   ],
   "source": [
    "instance_sequences = []\n",
    "for gen in generations: \n",
    "\n",
    "    sequences = gen.sequences\n",
    "\n",
    "    counter = 0\n",
    "    for i in range(0, sequences.shape[0], generation_config[\"num_return_sequences\"]):\n",
    "        instance_sequences.append(sequences[i:i+generation_config[\"num_return_sequences\"]])\n",
    "        counter += 1 \n",
    "\n",
    "    assert counter == batch_size\n",
    "\n",
    "print(len(instance_sequences), instance_sequences[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We filteded 10 unique correct complets\n"
     ]
    }
   ],
   "source": [
    "unique_correct_completions,incorrect_completions,unique_correct_completions_eqs =\\\n",
    "    generation._filter_completions(instance_sequences[0], TrainData.max_length_question, tokenizer)\n",
    "print(f\"We filteded {len(unique_correct_completions)} unique correct complets\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best index: 1\n",
      "Worst index: 0\n",
      "Gap: 0.765527950310559\n"
     ]
    }
   ],
   "source": [
    "best_idx, best_worst_idx, gap = generation._socre_equations(unique_correct_completions_eqs, unique_correct_completions)\n",
    "print(f\"Best index: {best_idx}\")\n",
    "print(f\"Worst index: {best_worst_idx}\")\n",
    "print(f\"Gap: {gap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Natalia sold clips to 48 of her friends in April and half as many in May, so the total number of clips sold can be represented by the equation: 48 (April sales) + (48/2) (May sales) = 48 + 24 = 72 #### 72\n",
      "****************************************************************************************************\n",
      "1\n",
      "Natalia sold clips to 48 friends in April and half as many in May, so the equations are:\n",
      "\n",
      "April sales: 48\n",
      "May sales: 48/2\n",
      "\n",
      "Total sales (April + May): 48 + (48/2)\n",
      "\n",
      "Simplifying the equation:\n",
      "\n",
      "Total sales = 48 + 24\n",
      "\n",
      "Total sales = 72\n",
      "\n",
      "#### 72\n",
      "****************************************************************************************************\n",
      "2\n",
      "Natalia sold clips to 48 friends in April and half as many in May, so the equation representing the total number of clips sold in April and May is: 48 + (48/2)\n",
      "\n",
      "Calculating the total: 48 + 24 = 72\n",
      "\n",
      "#### 72\n",
      "****************************************************************************************************\n",
      "3\n",
      "Natalia sold clips to 48 friends in April, and then sold half as many in May, which means she sold 48/2 = 24 clips in May. To find the total number of clips sold in April and May, we add the two amounts together: 48 (April) + 24 (May) = 72 clips.\n",
      "\n",
      "Equation: 48 + 24 = 72 #### 72\n",
      "****************************************************************************************************\n",
      "4\n",
      "Natalia sold clips to 48 friends in April and half as many in May, so the equation for May is 48/2. To find the total number of clips sold in April and May, we add the numbers together: 48 + (48/2)\n",
      "\n",
      "The final equation and answer is: 48 + (48/2) = 48 + 24 = 72 #### 72\n",
      "****************************************************************************************************\n",
      "5\n",
      "Natalia sold clips to 48 friends in April, and half as many in May, so the equation is: 48 (April) + (48/2) (May) = 48 + 24 = ####72 ####72\n",
      "****************************************************************************************************\n",
      "6\n",
      "Natalia sold clips to 48 friends in April, and then sold half as many in May, so the equation representing the total number of clips sold in April and May is: 48 (April sales) + (1/2)*48 (May sales)\n",
      "\n",
      "Solving the equation: 48 + (1/2)*48 = 48 + 24 = 72\n",
      "\n",
      "#### 72\n",
      "****************************************************************************************************\n",
      "7\n",
      "Natalia sold clips to 48 friends in April and half as many in May, so the equation representing the total number of clips sold in April and May is:\n",
      "\n",
      "48 + (48/2) = 48 + 24\n",
      "\n",
      "#### 72\n",
      "****************************************************************************************************\n",
      "8\n",
      "Natalia sold clips to 48 friends in April, and half as many in May, so the equation is: 48 (April sales) + (48/2) (May sales) = 48 + 24 = 72 #### 72\n",
      "****************************************************************************************************\n",
      "9\n",
      "Natalia sold clips to 48 friends in April, and half as many in May, so the equation for May's sales is 48/2. To find the total number of clips sold in April and May, we add the sales of April (48) to the sales of May (48/2):\n",
      "\n",
      "48 + 48/2 = 48 + 24 = 72\n",
      "\n",
      "Thus, Natalia sold 72 clips altogether in April and May. #### 72\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(unique_correct_completions)):\n",
    "    print(i)\n",
    "    print(unique_correct_completions[i])\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that the best completion selected is indeed better in terms of the structure and reasoning steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-2-End generation based on trainset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7473 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "#!accelerate launch --multi-gpu generation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'favored_solutions', 'infavored_solutions', 'wrong_solutions', 'favored_infavored_gaps'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntehetic_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12 * (50/60)) #### 10\n",
      "\n",
      "Weng earns $12 per hour and did 50 minutes of babysitting. To find the earnings, we convert 50 minutes to hours by dividing by 60 (since there are 60 minutes in an hour) and then multiply by her hourly rate of $12.\n",
      "\n",
      "Final answer: 10 ####\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "print(syntehetic_dataset[\"wrong_solutions\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n"
     ]
    }
   ],
   "source": [
    "print(syntehetic_dataset[\"question\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, we need to convert the 50 minutes of babysitting into hours since Weng earns per hour. There are 60 minutes in an hour, so:\n",
      "\n",
      "50 minutes Ã· 60 = 0.8333 hours (rounded to 4 decimal places)\n",
      "\n",
      "Now, we can multiply the number of hours she worked by her hourly rate:\n",
      "\n",
      "0.8333 hours Ã— $12/hour = $10.00 (rounded to 2 decimal places)\n",
      "\n",
      "The equation in a single line: 0.8333 Ã— 12 = 10.00\n",
      "\n",
      "#### 10.00\n"
     ]
    }
   ],
   "source": [
    "print(syntehetic_dataset[\"favored_solutions\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natalia sold clips to 48 friends in April, and then sold half as many in May, which is 48/2. To find the total number of clips sold in April and May, we add the two amounts together: 48 + (48/2)\n",
      "\n",
      "#### 72\n"
     ]
    }
   ],
   "source": [
    "print(syntehetic_dataset[\"infavored_solutions\"][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eynvcw7sdau5"
   },
   "source": [
    "# Training\n",
    "***\n",
    "\n",
    "Employ whatever trick you would like to reduce the VRAM requirements during training (including swapping the model for a smaller one, although please only as a last resort)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlAK9jEqkqEm"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    lr: float = 3e-5\n",
    "    epochs: int = 2\n",
    "    batch_size: int = 4\n",
    "    device: str = 'cpu'\n",
    "\n",
    "def train(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    dataset: datasets.Dataset,\n",
    "    config: TrainConfig,\n",
    ") -> AutoModelForCausalLM:\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        # Implement this\n",
    "        return\n",
    "\n",
    "    def loss_fn(batch):\n",
    "        # Implement an appropriate loss - note we don't expect this to necessarily\n",
    "        # be tied to the earlier mentioned paper, just something that is sensible\n",
    "        return\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=config.lr)\n",
    "\n",
    "    dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    # This is a pretty bare-bones loop, feel free to add anything particularly useful\n",
    "    for epoch in config.epochs:\n",
    "        model.train()\n",
    "\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = loss_fn(**batch)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "amazing_model = train(model, tokenizer, synthetic_dataset, TrainConfig())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tl8nfP8Hdsxf"
   },
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "This final part is more free-form. We'd like to evaluate our new model on the test set to see if it's improved, but then spend however much time you have left examining the model more closely / demonstrating some interesting behaviour / showing off beautiful plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHZRLUzCuYCg"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: AutoModelForCausalLM,\n",
    "    eval_dataset: datasets.Dataset,\n",
    ") -> float:\n",
    "    return 0.0\n",
    "\n",
    "our_score = evaluate_model(amazing_model, ds['test'])\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3.5-mini-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "their_score = evaluate_model(original_model, ds['test'])\n",
    "\n",
    "conclusion = 'ðŸŽ‰ðŸŽ‰ðŸŽ‰' if our_score > their_score else 'oh well, was it even supposed to work?'\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7vm1ZTHw8E1"
   },
   "source": [
    "### [Optional] - Discussion\n",
    "\n",
    "We would be interested to know:\n",
    "\n",
    "1.   If you were less time / computationally constrained, what would you do differently?\n",
    "2.   What would your ideal first project look like if you joined?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXIw-pCiwGRk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
