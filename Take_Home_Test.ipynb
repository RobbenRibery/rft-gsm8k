{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_jcCK5ncrsw"
   },
   "source": [
    "# Goal\n",
    "\n",
    "We'd like to learn a little more about how you practically approach a small research-like project loosely based on Rejection Sampling Fine-tuning (aka RFT, introduced in https://arxiv.org/abs/2308.01825).\n",
    "\n",
    "Tip: focus on section 3.3 (\"Rejection Sampling Fine-tuning\"). The paper isn't the best written, and we're happy to clarify anything.\n",
    "\n",
    "We will provide some skeleton code for you to guide what we would like to see from you, although if you have ideas for a different structure you feel is better or more elegant, then feel free to rewrite and replace at will.\n",
    "\n",
    "Note: your final submission does not have to be in a colab notebook, does not have to use Hugging Face, etc.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "We want to give you a chance to show off some of your best abilities.\n",
    "\n",
    "For some people that might mean generating high quality data in a smart way. For others, it might be speeding up the whole process to enable easy reproducibility, and maybe organizing the code in a better way than given. Yet for others, it might be a chance to show off some modern policy optimization techniques like DPO or its variants. Or maybe focusing on solid evaluations and identifying limitations of small models and limited fine-tuning.\n",
    "\n",
    "An ideal outcome of course is some sense of the model improving its mathematical abilities, but itâ€™s not a bad thing if the final evaluation somehow shows equal or worse performance ðŸ˜‚ (negative results are results).\n",
    "\n",
    "Ask lots of question! We're happy to answer any questions about the assignment, and to discuss concepts like RFT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojrmHoW2dLdU"
   },
   "source": [
    "# Setup [ignore - just run]\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "zPfAkbvHdEwJ"
   },
   "outputs": [],
   "source": [
    "#!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os  \n",
    "import torch\n",
    "import random \n",
    "import datasets\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from data import GSM8KDataset, _apply_template\n",
    "from prompt import EvalTemplate\n",
    "\n",
    "import utils\n",
    "import generation \n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "SEED = 128 \n",
    "MODEL_NAME = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "# set seeds\n",
    "torch.random.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3150128302f640b797f17122d5dc7a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Training instances: 7473\n",
      "Num Validation instances: 1319\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16, #Â accelerate inf \n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_dataset = datasets.load_dataset('gsm8k', 'main')['train']\n",
    "val_dataset = datasets.load_dataset('gsm8k', 'main')['test'] \n",
    "print(f\"Num Training instances: {train_dataset.shape[0]}\")\n",
    "print(f\"Num Validation instances: {val_dataset.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# for i in tqdm(range(len(gen_dataset_2))):\n",
    "#     for j in range(len(train_dataset)):\n",
    "#         if gen_dataset_2[i]['question'] == train_dataset[j]['question']:\n",
    "#             assert utils.GSM8KParser.get_answer_from_gt(train_dataset[j]['answer']) == \\\n",
    "#                  utils.GSM8KParser.get_answer_from_pred(gen_dataset_2[i]['favored_solutions']), print(i, j)\n",
    "# verifcation, this cell takes about 7mins to run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question\n",
      "Five coworkers were talking during the lunch break. Roger, the oldest one, said that he has the same amount of experience in years as all four of the others combined and that his retirement should come when he accumulates 50 years of experience. Peter said that when he came to the company his daughter was 7 years old, and now she is 19 years old. Tom then said he has twice as many years of experience as Robert. Robert said that he has 4 years of experience less than Peter but 2 more years of experience than Mike. How many more years does Roger have to work before he retires?\n",
      "question_idx\n",
      "512\n",
      "favored_solutions\n",
      "Let's denote the years of experience of Roger, Peter, Tom, Robert, and Mike as R, P, T, Ro, and Mi respectively.\n",
      "\n",
      "1. R = P + T + Ro + Mi (Roger's experience is the sum of the others)\n",
      "2. R + (P - 12) = 50 (Peter's experience is 19 - 7 = 12 years more than his daughter's current age)\n",
      "3. T = 2Ro (Tom has twice as many years of experience as Robert)\n",
      "4. Ro = P - 4 (Robert has 4 years less than Peter)\n",
      "5. Ro = Mi + 2 (Robert has 2 years more than Mike)\n",
      "\n",
      "Now we can solve these equations step by step:\n",
      "\n",
      "From equation (4), we have P = Ro + 4.\n",
      "\n",
      "Substituting P in equation (1), we get R = (Ro + 4) + T + Ro + Mi.\n",
      "\n",
      "Simplifying, we have R = 2Ro + T + Mi + 4.\n",
      "\n",
      "Now, we substitute T from equation (3), so we get R = 2Ro + 2Ro + Mi + 4.\n",
      "\n",
      "Simplifying further, we have R = 4Ro + Mi + 4.\n",
      "\n",
      "Using equation (5), we can substitute Mi with Ro - 2, so we get R = 4Ro + (Ro - 2) + 4.\n",
      "\n",
      "Simplifying, we have R = 5Ro + 2.\n",
      "\n",
      "Now, we substitute R from equation (2), so we get 5Ro + 2 = 50 - 12.\n",
      "\n",
      "Simplifying, we have 5Ro + 2 = 38.\n",
      "\n",
      "Subtracting 2 from both sides, we get 5Ro = 36.\n",
      "\n",
      "Dividing by 5, we have Ro = 36 / 5 = 7.2.\n",
      "\n",
      "Now, we can find T using equation (3): T = 2 * 7.2 = 14.4.\n",
      "\n",
      "And we can find Mi using equation (5): Mi = 7.2 - 2 = 5.2.\n",
      "\n",
      "Finally, we can find P using equation (4): P = 7.2 + 4 = 11.2.\n",
      "\n",
      "Now we have all the values, we can find Roger's remaining years to retirement:\n",
      "\n",
      "R + (P - 12) = 50\n",
      "R + (11.2 - 12) = 50\n",
      "R - 0.8 = 50\n",
      "R = 50 + 0.8\n",
      "R = 50.8\n",
      "\n",
      "Roger has 50.8 years of experience, so he needs 50 - 50.8 = -0.8 more years to retire. Since the answer must be in pure digits without negative sign, we take the absolute value: 0.8.\n",
      "\n",
      "#### 8\n",
      "infavored_solutions\n",
      "Let's denote the experience of Roger, Peter, Tom, Robert, and Mike as R, P, T, Ro, and Mi, respectively.\n",
      "\n",
      "1. Roger's experience is the sum of the experiences of all four coworkers: R = P + T + Ro + Mi\n",
      "2. Peter's experience is his daughter's current age minus her age when he joined: P = 19 - 7 = 12 years\n",
      "3. Robert has 4 years less experience than Peter: Ro = P - 4 = 12 - 4 = 8 years\n",
      "4. Robert has 2 more years of experience than Mike: Mi = Ro - 2 = 8 - 2 = 6 years\n",
      "5. Tom has twice as many years of experience as Robert: T = 2 * Ro = 2 * 8 = 16 years\n",
      "\n",
      "Now, we substitute the values of P, Ro, and Mi into the first equation:\n",
      "\n",
      "R = 12 + 16 + 8 + 6\n",
      "R = 42 years\n",
      "\n",
      "Roger needs 50 years of experience to retire, so the number of years he has to work before retirement is:\n",
      "\n",
      "50 - 42 = 8 years\n",
      "\n",
      "#### 8\n",
      "wrong_solutions\n",
      "Let R be Roger's years of experience, P be Peter's, T be Tom's, and M be Mike's.\n",
      "\n",
      "From the given information, we have:\n",
      "\n",
      "1. R = P + T + M (Roger has the same amount of experience as all others combined)\n",
      "2. R + P + T + M + M = 50 (Roger's retirement goal)\n",
      "3. Peter's daughter was 7 years old when he joined and is now 19, so P = 19 - 7 = 12\n",
      "4. T = 2R (Tom has twice as many years of experience as Robert)\n",
      "5. P = R + 4 (Peter has 4 years more experience than Robert)\n",
      "6. M = R - 2 (Mike has 2 years less experience than Robert)\n",
      "\n",
      "Now we substitute equations (3), (4), and (6) into equation (1):\n",
      "\n",
      "R = (R + 4) + 2(R) + (R - 2)\n",
      "\n",
      "Simplify and solve for R:\n",
      "\n",
      "R = R + 4 + 2R + R - 2\n",
      "0 = 4R + 2\n",
      "-2 = 4R\n",
      "R = -2 / 4\n",
      "R = -0.5\n",
      "\n",
      "However, negative years of experience are not possible. There must be a mistake in our equation setup or calculations. Let's re-examine the information given:\n",
      "\n",
      "Equation (5) should be R - 4 = P (Peter has 4 years less experience than Roger).\n",
      "\n",
      "Now, let's substitute the correct equations into equation (1):\n",
      "\n",
      "R = (R - 4) + 2(R - 2) + (R - 2)\n",
      "\n",
      "Simplify and solve for R:\n",
      "\n",
      "R = R - 4 + 2R - 4 + R - 2\n",
      "0 = 4R - 10\n",
      "10 = 4R\n",
      "R = 10 / 4\n",
      "R = 2.5\n",
      "\n",
      "Since Roger has 2.5 years of experience, he needs to work for 50 - 2.5 = 47.5 more years before retiring.\n",
      "\n",
      "#### 47.5\n",
      "favored_infavored_gaps\n",
      "832.0\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "utils.inspect_instance(gen_dataset_2, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question\n",
      "Ian used a grocery delivery app to have his groceries delivered.  His original order was $25 before delivery and tip.  He noticed that 3 items changed on his order.  A $0.99 can of tomatoes was replaced by a $2.20 can of tomatoes, his $1.00 lettuce was replaced with $1.75 head of lettuce and his $1.96 celery was replaced with celery that cost $2.00.  Delivery and tip came to a total of $8.00.  How much is his new bill now, with the food substitutes and delivery/tip?\n",
      "question_idx\n",
      "588\n",
      "favored_solutions\n",
      "First, we calculate the difference in cost for each item changed:\n",
      "$2.20 (new can of tomatoes) - $0.99 (old can of tomatoes) = $1.21\n",
      "$1.75 (new head of lettuce) - $1.00 (old lettuce) = $0.75\n",
      "$2.00 (new celery) - $1.96 (old celery) = $0.04\n",
      "\n",
      "Next, we add the differences together to find the total increase in cost due to the substitutions:\n",
      "$1.21 + $0.75 + $0.04 = $2.00\n",
      "\n",
      "Now, we add the total increase in cost to the original order amount:\n",
      "$25.00 (original order) + $2.00 (increase due to substitutions) = $27.00\n",
      "\n",
      "Finally, we add the delivery and tip cost to the new order amount:\n",
      "$27.00 (new order amount) + $8.00 (delivery and tip) = $35.00\n",
      "\n",
      "So, the final answer is #### 35.\n",
      "infavored_solutions\n",
      "First, calculate the difference in cost for each item that changed:\n",
      "\n",
      "Tomatoes: $2.20 - $0.99 = $1.21\n",
      "Lettuce: $1.75 - $1.00 = $0.75\n",
      "Celery: $2.00 - $1.96 = $0.04\n",
      "\n",
      "Now, add the differences together to find the total increase in cost:\n",
      "\n",
      "$1.21 + $0.75 + $0.04 = $2.00\n",
      "\n",
      "Finally, add the total increase to the original order and include the delivery and tip:\n",
      "\n",
      "$25.00 + $2.00 + $8.00 = #### 35.00\n",
      "\n",
      "Final answer: #### 35\n",
      "wrong_solutions\n",
      "First, calculate the difference in cost for each item that was replaced:\n",
      "- Tomatoes: $2.20 - $0.99 = $1.21\n",
      "- Lettuce: $1.75 - $1.00 = $0.75\n",
      "- Celery: $2.00 - $1.96 = $0.04\n",
      "\n",
      "Now, add the differences together to find the total increase in cost due to the substitutions:\n",
      "$1.21 + $0.75 + $0.04 = $1.99\n",
      "\n",
      "Next, add the delivery and tip cost to the original order and the increase in cost due to substitutions:\n",
      "$25.00 + $1.99 + $8.00 = $34.99\n",
      "\n",
      "So, Ian's new bill is $34.99.\n",
      "\n",
      "Final answer: #### 3499\n",
      "favored_infavored_gaps\n",
      "262.0\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "gen_dataset = datasets.load_from_disk(\"corrected-pred-parser-0.5lev-gsm8k_synthetic_data_747instances_5samples\")\n",
    "gen_dataset = gen_dataset.sort(column_names=\"favored_infavored_gaps\", reverse=True)\n",
    "utils.inspect_instance(gen_dataset, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question\n",
      "Susy goes to a large school with 800 students, while Sarah goes to a smaller school with only 300 students.  At the start of the school year, Susy had 100 social media followers.  She gained 40 new followers in the first week of the school year, half that in the second week, and half of that in the third week.  Sarah only had 50 social media followers at the start of the year, but she gained 90 new followers the first week, a third of that in the second week, and a third of that in the third week.  After three weeks, how many social media followers did the girl with the most total followers have?\n",
      "question_idx\n",
      "2\n",
      "favored_solutions\n",
      "Let's calculate the total followers for both girls after three weeks:\n",
      "\n",
      "Susy's followers:\n",
      "Initial followers: 100\n",
      "First week: +40\n",
      "Second week: +40/2 = +20\n",
      "Third week: +20/2 = +10\n",
      "Total followers for Susy: 100 + 40 + 20 + 10 = 170\n",
      "\n",
      "Sarah's followers:\n",
      "Initial followers: 50\n",
      "First week: +90\n",
      "Second week: +90/3 = +30\n",
      "Third week: +30/3 = +10\n",
      "Total followers for Sarah: 50 + 90 + 30 + 10 = 180\n",
      "\n",
      "Now, we compare the total followers to find the girl with the most followers:\n",
      "\n",
      "170 (Susy) vs. 180 (Sarah)\n",
      "\n",
      "Sarah has more followers.\n",
      "\n",
      "#### 180\n",
      "infavored_solutions\n",
      "Let's calculate the total followers for Susy and Sarah after three weeks:\n",
      "\n",
      "Susy:\n",
      "Week 1: 100 + 40 = 140 followers\n",
      "Week 2: 140 + 40/2 = 140 + 20 = 160 followers\n",
      "Week 3: 160 + 20/2 = 160 + 10 = 170 followers\n",
      "Total followers for Susy: 170\n",
      "\n",
      "Sarah:\n",
      "Week 1: 50 + 90 = 140 followers\n",
      "Week 2: 140 + 90/3 = 140 + 30 = 170 followers\n",
      "Week 3: 170 + 30/3 = 170 + 10 = 180 followers\n",
      "Total followers for Sarah: 180\n",
      "\n",
      "Comparing the totals, Sarah has more followers after three weeks (180 > 170).\n",
      "\n",
      "Final answer: #### 180\n",
      "wrong_solutions\n",
      "<NOWRONG SOLUTION>\n",
      "favored_infavored_gaps\n",
      "0.32397959183673475\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "idx = 2\n",
    "\n",
    "utils.inspect_instance(gen_dataset, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question\n",
      "Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\n",
      "answer\n",
      "In the beginning, Betty has only 100 / 2 = $<<100/2=50>>50.\n",
      "Betty's grandparents gave her 15 * 2 = $<<15*2=30>>30.\n",
      "This means, Betty needs 100 - 50 - 30 - 15 = $<<100-50-30-15=5>>5 more.\n",
      "#### 5\n",
      "question_length\n",
      "64\n",
      "answer_length\n",
      "107\n",
      "num_hops\n",
      "3\n",
      "answer_str_digit\n",
      "5\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_dataset)):\n",
    "    if train_dataset[i]['question'] == gen_dataset[idx]['question']:\n",
    "        utils.inspect_instance(train_dataset, i)\n",
    "        print(utils.GSM8KParser.get_answer_from_gt(gen_dataset[idx]['favored_solutions']))\n",
    "        print(utils.GSM8KParser.get_answer_from_gt(train_dataset[i]['answer']))\n",
    "        print(utils.GSM8KParser.get_answer_from_pred(train_dataset[i]['answer']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "****\n",
    "- The reasoning paths/hops seem to present in seperate lines (leading to easiniess in parsing them)\n",
    "- The equaiton tags ```<< >>``` are used to train language models to invoke calculators in the original GSM8K OpenAI paper.\n",
    "- **Verified that our ```utils.GSM8KParser.get_answer_from_pred``` is parsing the same result as the ground truth parser**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 7473\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Checking instance 3282:\n",
      "question\n",
      "Itâ€™s February 2021.  Mark was born in January 1976.  Graham is 3 years younger than Mark, and Grahamâ€™s sister, Janice, is 1/2 the age of Graham.  How old is Janice?\n",
      "answer\n",
      "Itâ€™s 2021 and Mark was born in 1976 so Mark is 2021-1976 = <<2021-1976=45>>45 years old\n",
      "Graham is 3 years younger than Mark who is 45 so Graham is 45-3 = 42 years old\n",
      "Janice is 1/2 the age of Graham who is 42 so Janice is 42/2 = <<42/2=21>>21 years old\n",
      "#### 21\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Checking instance 7251:\n",
      "question\n",
      "Melissa sells a coupe for $30,000 and an SUV for twice as much. If her commission is 2%, how much money did she make from these sales?\n",
      "answer\n",
      "First find the total cost of the SUV: $30,000 * 2 = $<<30000*2=60000>>60,000\n",
      "Then add the cost of the coupe to find the total cost of the cars: $60,000 + $30,000 = $<<60000+30000=90000>>90,000\n",
      "Multiplying that amount by Melissa's commission rate, we find her earnings are $90,000 * 2% = $<<90000*2*.01=1800>>1800\n",
      "#### 1800\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Checking instance 5605:\n",
      "question\n",
      "Two years ago, Jared was twice as old as Tom. If Tom will be 30 in five years, how old is Jared now?\n",
      "answer\n",
      "Tom will be 30 in 5 years so now he is 30-5 = <<30-5=25>>25\n",
      "2 years ago Tom was 25-2 = <<25-2=23>>23\n",
      "At that time, Jared was twice as old as Tom so Jared was 23*2 = 46\n",
      "Two years have since passed so Jared is now 46+2 = <<46+2=48>>48\n",
      "#### 48\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Checking instance 2386:\n",
      "question\n",
      "An animal shelter recently took in twelve cats. The shelter already had half that number of cats. Two days later, three had been adopted. The next day, one of the cats had five kittens, and one person arrived to pick up one of the cats that was their missing pet. How many cats does the shelter have now?\n",
      "answer\n",
      "The shelter started with 12 / 2 = <<12/2=6>>6 cats.\n",
      "The cats they took in increased the number to 12 + 6 = <<12+6=18>>18 cats.\n",
      "Three cats were adopted, so they had 18 - 3 = <<18-3=15>>15 cats.\n",
      "One cat had 5 kittens, which increased the number of cats to 15 + 5 = 20 cats.\n",
      "The pet owner came to get his missing cat, so they now have 20 - 1 = <<20-1=19>>19 cats.\n",
      "#### 19\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Checking instance 7287:\n",
      "question\n",
      "Lena has 16 candy bars. She needs 5 more candy bars to have 3 times as many as Kevin, and Kevin has 4 candy bars less than Nicole. How many more candy bars does Lena have than Nicole?\n",
      "answer\n",
      "Kevin has (16 + 5)/3 = <<(16+5)/3=7>>7 candy bars.\n",
      "Nicole has 7 + 4 = <<7+4=11>>11 candy bars.\n",
      "Lena has 16 - 11 = <<16-11=5>>5 more candy bars than Nicole\n",
      "#### 5\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    seed = np.random.randint(0, len(train_dataset))\n",
    "    print(\"*\"*100)\n",
    "    print(f\"Checking instance {seed}:\")\n",
    "    utils.inspect_instance(train_dataset, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only look at train set now for certain information that will be used\n",
    "during inference \n",
    "\n",
    "- Maximum length (num_tokens) of question: 239\n",
    "- Maximum length (num_tokens) of answer: 475 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum answer num_tokens: 475\n",
      "Maximum question num_tokens: 239\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    lambda x: utils.GSM8KParser.get_question_length(x['question'], tokenizer)\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x: utils.GSM8KParser.get_answer_length(x['answer'], tokenizer) \n",
    ")\n",
    "print(f\"Maximum answer num_tokens: {max(train_dataset['answer_length'])}\")\n",
    "print(f\"Maximum question num_tokens: {max(train_dataset['question_length'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04278a563ae14af18a8a376ffea30f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22aa86eb4f954363b1e8c68503b849a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Â infer number of hops \n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x: utils.GSM8KParser.get_num_hops(x['answer'])\n",
    ")\n",
    "\n",
    "# infer answes using ground truth parser \n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x: utils.GSM8KParser.get_answer_from_gt(x['answer'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optinal Cell (Only to verify that parsing from \n",
    "# ground truth and parsing from completion would \n",
    "# yield the same result \n",
    "# infer answers using prediction parser\n",
    "answer_str_inf = [\n",
    "    utils.GSM8KParser.get_answer_from_pred(x)['answer_str_digit'] \\\n",
    "    for x in train_dataset['answer']\n",
    "]\n",
    "assert answer_str_inf == train_dataset['answer_str_digit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on the collate function \n",
    "Instead of tokenizing the dataset using ```collate_fn``` on the fly. I went for pre-tokenzing the dataset into a static format, avoiding the need to tokenzie data on the fly. This would lead to faster training runs, but less flexibility in terms of handling data on the fly. i.e. If you want to slightly modify the quesiton conditional on the model performance during training, it's impossible to do it without a collate function.\n",
    "\n",
    "However, since our problem is simple, we can afford to pre-tokenize the dataset.\n",
    "\n",
    "That said, there is a dummy collation function ```class PreprocessedCollator(DataCollatorMixin)``` in ```lora.py```, simply offloading the input to format required by SFTTRainer. \n",
    "\n",
    "Inside ```data.py```, the tokenization is done in the ```GSM8KDataset``` class, which runs a ````self._preprocess```` function on instentiation. \n",
    "\n",
    "All required attributes, i.e. input_ids, attention_mask, labels are prepared once instentiation completes. For detailed explanation, please refer to ```data.GSM8KDataset._preprocess```. \n",
    "\n",
    "If we refers to ```data.GSM8KDataset``` class, you could find that the output schema is rather long, which could result in taking up nunecessary spaces during training, further engineering effort could be inplace to remediate that. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4451cd94554d0bbca311cf33e410c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0fe18bf568421ba8cee327b699e812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ed699a20fe403983a2f95bb59366e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b765b84c12d543dab75d2d47a5308f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43ab9c4e63748f89b155121dbc897b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum answer num_tokens: 477\n",
      "Maximum question num_tokens: 359\n",
      "Maximum sequence num_tokens: 836\n",
      "Maximum new tokens in generation: 527\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f0855370364a23a7a2cc0ce9a7cc66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Completed dataset:\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'num_hops', 'answer_str_digit', 'formatted_question', 'formatted_answer', 'question_length', 'answer_length', 'question_input_ids', 'question_attention_mask', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 7473\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.load_dataset('gsm8k', 'main')['train']\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x: utils.GSM8KParser.get_num_hops(x['answer'])\n",
    ")\n",
    "TrainData = GSM8KDataset(train_dataset, tokenizer) #Complete tokenization of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c5c14b80bb43d994429f66539d362a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62351031a84a4453bb56a69b8be8d1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4dd2a6eac434f0bbc0add421c2ca8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7faf1e404a214a46841f2868cfd5cdac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f192f989a3641b19962e507d3908428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum question num_tokens: 310\n",
      "Maximum answer num_tokens: 430\n",
      "Maximum sequence num_tokens: 740\n",
      "Maximum new tokens in generation: 480\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799500233aeb4b618502e57b4fcc8b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Completed dataset:\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'num_hops', 'answer_str_digit', 'formatted_question', 'formatted_answer', 'question_length', 'answer_length', 'question_input_ids', 'question_attention_mask', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1319\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "val_dataset = datasets.load_dataset('gsm8k', 'main')['test'] \n",
    "val_dataset = val_dataset.map(\n",
    "    lambda x: utils.GSM8KParser.get_num_hops(x['answer'])\n",
    ")\n",
    "valData = GSM8KDataset(val_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation \n",
    "\n",
    "* We wanted to validate that our mannual padding is done correctly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dataloader = DataLoader(TrainData, batch_size=2, shuffle=False)\n",
    "for batch in dummy_dataloader:\n",
    "    assert batch['input_ids'].shape[0] == 2\n",
    "    assert (batch['input_ids'] == TrainData[:2]['input_ids']).all()\n",
    "    break \n",
    "# first we validated that it's loading a homugenous batch of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding for the entire sequence (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|> You are a highly intelligent assistant who is exceptional at solving Math Problems.<|end|><|user|> *Task*    \n",
      "Think step by step to solve the following question:\n",
      "NOTE:\n",
      "1. Reason deductively.\n",
      "1. Write all equations in a single line wihtout breaks in the middle.\n",
      "2. Submit final answer using PURE DIGITS, in the last starting with \"####\".\n",
      "    i.e. \"#### 10\" if the final answer is $10\n",
      "\n",
      "```question\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "```<|end|><|assistant|> Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "#### 72<|end|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "instance = TrainData[0]\n",
    "#Â check (attention mask == 1) part for the entire sequence\n",
    "print(tokenizer.decode(\n",
    "    instance[\"input_ids\"][instance['attention_mask']!=0]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "#### 72<|end|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# check (labels!= -100) part for the entire sequence\n",
    "print(tokenizer.decode(\n",
    "    instance[\"input_ids\"][instance['labels']!=-100]\n",
    "))\n",
    "# noticed how loss is not measured on <|assistant|>, it belongs to part of the question "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding for the question only (inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|> You are a highly intelligent assistant who is exceptional at solving Math Problems.<|end|><|user|> *Task*    \n",
      "Think step by step to solve the following question:\n",
      "NOTE:\n",
      "1. Reason deductively.\n",
      "1. Write all equations in a single line wihtout breaks in the middle.\n",
      "2. Submit final answer using PURE DIGITS, in the last starting with \"####\".\n",
      "    i.e. \"#### 10\" if the final answer is $10\n",
      "\n",
      "```question\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "```<|end|><|assistant|>\n"
     ]
    }
   ],
   "source": [
    "# check (attention_mask = 1) part for the question \n",
    "print(tokenizer.decode(\n",
    "    instance[\"question_input_ids\"][instance[\"question_attention_mask\"]!=0]\n",
    "))\n",
    "#Â noticed how the question_input_ids only include everything up to <|assistant>| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Vibe Check \n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): Phi3ForCausalLM(\n",
       "    (model): Phi3Model(\n",
       "      (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "      (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x Phi3DecoderLayer(\n",
       "          (self_attn): Phi3FlashAttention2(\n",
       "            (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "            (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "            (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Phi3MLP(\n",
       "            (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "            (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "            (activation_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Phi3RMSNorm()\n",
       "          (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (post_attention_layernorm): Phi3RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): Phi3RMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config = {\n",
    "    \"max_new_tokens\" : 1024,\n",
    "    \"temperature\": 0.1, \n",
    "    \"num_return_sequences\":1,\n",
    "    \"top_p\": 0.9,\n",
    "    \"eos_token_id\":tokenizer.eos_token_id,  # Specify the EOS token\n",
    "    \"pad_token_id\":tokenizer.eos_token_id, \n",
    "    \"do_sample\":True,\n",
    "    \"output_scores\":False,\n",
    "    \"return_dict_in_generate\":True,\n",
    "}\n",
    "model = torch.compile(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate instances \n",
    "* Test the \"easiest\" and the \"hardest\" instances from GSM8K, ranked according to the number of hops \n",
    "required to solve the problem \n",
    "\n",
    "* Noticed that the equation parser `GSM8KParser.parse_equations_from_pred` is not perfectly working, but it can extract sequences of numers and operatiing signs inside each lines of the text. Still giving us information about the numerical operations that has been performed. \n",
    "\n",
    "* Whether we should include text when parsing the eqaution is a question that we could investigate \n",
    "(In different cases, we could see the advantage and )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shortest hop required instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natalia sold clips to 48 friends in April and half as many in May, which means she sold 48/2 = 24 clips in May. To find the total number of clips sold in April and May, we add the two amounts together: 48 (April) + 24 (May) = 72. Therefore, the final answer is ####72.\n"
     ]
    }
   ],
   "source": [
    "## question that would need to be solved by the shortest hope \n",
    "sorted_data = sorted(TrainData, key=lambda x: x[\"num_hops\"])\n",
    "instance = sorted_data[0]\n",
    "\n",
    "chats = [instance[\"formatted_question\"]]\n",
    "responses = utils.sample_answers(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    chats,\n",
    "    **generation_config,\n",
    ")\n",
    "print(responses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction -> 72\n",
      "Label -> 72\n"
     ]
    }
   ],
   "source": [
    "answer = instance[\"answer_str_digit\"] \n",
    "prediction =  utils.GSM8KParser.get_answer_from_pred(responses[0])[\"answer_str_digit\"]\n",
    "\n",
    "print(f\"Prediction -> {prediction}\")\n",
    "print(f\"Label -> {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4848/2=2448+24=7272']\n"
     ]
    }
   ],
   "source": [
    "parsed_eqs = utils.GSM8KParser.parse_equations_from_pred(responses[0])\n",
    "print(parsed_eqs[\"equations\"])\n",
    "## the parser will keep looks for the previous digits before the equal sign until it stops,\n",
    "## which could be clamping all numbers and operational signs in a single line together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['48 friends in April and half as many in May, which means she sold 48/2 = 24 clips in May. To find the total number of clips sold in April and May, we add the two amounts together: 48 (April) + 24 (May) = 72. Therefore, the final answer is ####72']\n"
     ]
    }
   ],
   "source": [
    "parsed_eqs = utils.GSM8KParser.parse_equations_from_pred(responses[0], include_text=True)\n",
    "print(parsed_eqs[\"equations\"])\n",
    "## including the text could boil down to using the entire sequence, which is undesired "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longest hop required instnace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve this problem, we need to follow these steps:\n",
      "\n",
      "1. Determine the number of male and female students based on the given ratio and total number of students.\n",
      "2. Calculate the number of male and female students who like to play basketball.\n",
      "3. Find the total number of students who like to play basketball.\n",
      "4. Calculate the number of students who do not like to play basketball.\n",
      "5. Determine the percentage of students who do not like to play basketball out of the total student population.\n",
      "\n",
      "Let's go through these steps:\n",
      "\n",
      "Step 1: Determine the number of male and female students.\n",
      "The ratio of male to female students is 3:2, and there are 1000 students in total. To find out how many are male and how many are female, we can set up a proportion:\n",
      "\n",
      "Let \\( M \\) be the number of male students and \\( F \\) be the number of female students.\n",
      "\\[ \\frac{M}{F} = \\frac{3}{2} \\]\n",
      "\\[ M + F = 1000 \\]\n",
      "\n",
      "From the ratio, we can express \\( M \\) in terms of \\( F \\):\n",
      "\\[ M = \\frac{3}{2}F \\]\n",
      "\n",
      "Now we can substitute \\( M \\) in the total number of students equation:\n",
      "\\[ \\frac{3}{2}F + F = 1000 \\]\n",
      "\\[ \\frac{5}{2}F = 1000 \\]\n",
      "\\[ F = \\frac{1000 \\times 2}{5} \\]\n",
      "\\[ F = 400 \\]\n",
      "\n",
      "Now we can find \\( M \\):\n",
      "\\[ M = \\frac{3}{2} \\times 400 \\]\n",
      "\\[ M = 600 \\]\n",
      "\n",
      "So, there are 600 male students and 400 female students.\n",
      "\n",
      "Step 2: Calculate the number of male and female students who like to play basketball.\n",
      "For male students:\n",
      "\\[ \\frac{2}{3} \\times 600 = 400 \\]\n",
      "\n",
      "For female students:\n",
      "\\[ \\frac{1}{5} \\times 400 = 80 \\]\n",
      "\n",
      "Step 3: Find the total number of students who like to play basketball.\n",
      "\\[ 400 \\text{ (male students)} + 80 \\text{ (female students)} = 480 \\]\n",
      "\n",
      "Step 4: Calculate the number of students who do not like to play basketball.\n",
      "\\[ 1000 \\text{ (total students)} - 480 \\text{ (students who like basketball)} = 520 \\]\n",
      "\n",
      "Step 5: Determine the percentage of students who do not like to play basketball.\n",
      "\\[ \\frac{520}{1000} \\times 100\\% = 52\\% \\]\n",
      "\n",
      "Therefore, 52% of the student population do not like to play basketball.\n",
      "\n",
      "The final answer is \\boxed{52}.\n",
      "\n",
      "The answer is: 52. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# longest answer \n",
    "instance = sorted_data[-1]\n",
    "\n",
    "chats = [instance[\"question\"]]\n",
    "responses = utils.sample_answers(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    chats,\n",
    "    **generation_config,\n",
    ")\n",
    "print(responses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction -> <INVALID_ANSWER>\n",
      "Label -> 52\n"
     ]
    }
   ],
   "source": [
    "answer = instance[\"answer_str_digit\"] \n",
    "prediction =  utils.GSM8KParser.get_answer_from_pred(responses[0])[\"answer_str_digit\"]\n",
    "\n",
    "print(f\"Prediction -> {prediction}\")\n",
    "print(f\"Label -> {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5201000100=52', '1000-480=520', '23600=400', '400+80=480', '52=1000', '15400=80', '32+=1000']\n"
     ]
    }
   ],
   "source": [
    "parsed_eqs = utils.GSM8KParser.parse_equations_from_pred(responses[0])\n",
    "\n",
    "#Â this time the model addresses the problem in LaTex, which leads to only 2 ssuccessfull parsing\n",
    "print(parsed_eqs[\"equations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\\\frac{2}{3} \\\\times 600 = 400', '400 \\\\text{ (male students)} + 80 \\\\text{ (female students)} = 480', '\\\\frac{1}{5} \\\\times 400 = 80', '\\\\frac{5}{2}F = 1000', '1000 \\\\text{ (total students)} - 480 \\\\text{ (students who like basketball)} = 520', '\\\\frac{3}{2}F + F = 1000', '\\\\frac{520}{1000} \\\\times 100\\\\% = 52']\n"
     ]
    }
   ],
   "source": [
    "parsed_eqs = utils.GSM8KParser.parse_equations_from_pred(responses[0], include_text=True)\n",
    "\n",
    "#Â this time a text-inclusive parser provides much better extraction in terms of \n",
    "# numerical operations required \n",
    "print(parsed_eqs[\"equations\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data \n",
    "***\n",
    "- Here we generate 10 response per question using our base model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Walthrough "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we adjust the generation config \n",
    "# [1] bump up the temperature to increase the uncertainty, this will boost the diversity of the reasoning path\n",
    "# [2] since we are using top-p sampling, by increasing the temperature, we are potentially increasing the \n",
    "# number of tokens inside this top-p pool, which further diversifies the choices of tokens available for us in \n",
    "# each timestamp. We decrease top-p to 0 to further encourage diversify. \n",
    "#Â This is the same as suggested in the scaling paper \n",
    "\n",
    "num_sequence = 10\n",
    "generation_config = {\n",
    "    \"max_new_tokens\" : TrainData.inf_seq_length,\n",
    "    \"temperature\": 0.7,\n",
    "    \"num_return_sequences\":num_sequence,\n",
    "    #\"top_p\": 0.5,\n",
    "    \"eos_token_id\":tokenizer.eos_token_id,  # Specify the EOS token\n",
    "    \"pad_token_id\":tokenizer.eos_token_id, \n",
    "    \"do_sample\":True,\n",
    "    \"output_scores\":False,\n",
    "    \"return_dict_in_generate\":True,\n",
    "}\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3737 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3737 [00:09<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "generations = generation.get_generations(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    TrainData,\n",
    "    batch_size=batch_size,\n",
    "    **generation_config\n",
    ")\n",
    "# force only one generation with a break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 torch.Size([10, 480])\n"
     ]
    }
   ],
   "source": [
    "instance_sequences = []\n",
    "for gen in generations: \n",
    "\n",
    "    sequences = gen.sequences\n",
    "\n",
    "    counter = 0\n",
    "    for i in range(0, sequences.shape[0], generation_config[\"num_return_sequences\"]):\n",
    "        instance_sequences.append(sequences[i:i+generation_config[\"num_return_sequences\"]])\n",
    "        counter += 1 \n",
    "\n",
    "    assert counter == batch_size\n",
    "\n",
    "print(len(instance_sequences), instance_sequences[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We filteded 10 unique correct complets\n"
     ]
    }
   ],
   "source": [
    "unique_correct_completions,incorrect_completions,unique_correct_completions_eqs =\\\n",
    "    generation._filter_completions(instance_sequences[0], TrainData.max_length_question, tokenizer)\n",
    "print(f\"We filteded {len(unique_correct_completions)} unique correct complets\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best index: 1\n",
      "Worst index: 0\n",
      "Gap: 0.765527950310559\n"
     ]
    }
   ],
   "source": [
    "best_idx, best_worst_idx, gap = generation._socre_equations(unique_correct_completions_eqs, unique_correct_completions)\n",
    "print(f\"Best index: {best_idx}\")\n",
    "print(f\"Worst index: {best_worst_idx}\")\n",
    "print(f\"Gap: {gap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Natalia sold clips to 48 of her friends in April and half as many in May, so the total number of clips sold can be represented by the equation: 48 (April sales) + (48/2) (May sales) = 48 + 24 = 72 #### 72\n",
      "****************************************************************************************************\n",
      "1\n",
      "Natalia sold clips to 48 friends in April and half as many in May, so the equations are:\n",
      "\n",
      "April sales: 48\n",
      "May sales: 48/2\n",
      "\n",
      "Total sales (April + May): 48 + (48/2)\n",
      "\n",
      "Simplifying the equation:\n",
      "\n",
      "Total sales = 48 + 24\n",
      "\n",
      "Total sales = 72\n",
      "\n",
      "#### 72\n",
      "****************************************************************************************************\n",
      "2\n",
      "Natalia sold clips to 48 friends in April and half as many in May, so the equation representing the total number of clips sold in April and May is: 48 + (48/2)\n",
      "\n",
      "Calculating the total: 48 + 24 = 72\n",
      "\n",
      "#### 72\n",
      "****************************************************************************************************\n",
      "3\n",
      "Natalia sold clips to 48 friends in April, and then sold half as many in May, which means she sold 48/2 = 24 clips in May. To find the total number of clips sold in April and May, we add the two amounts together: 48 (April) + 24 (May) = 72 clips.\n",
      "\n",
      "Equation: 48 + 24 = 72 #### 72\n",
      "****************************************************************************************************\n",
      "4\n",
      "Natalia sold clips to 48 friends in April and half as many in May, so the equation for May is 48/2. To find the total number of clips sold in April and May, we add the numbers together: 48 + (48/2)\n",
      "\n",
      "The final equation and answer is: 48 + (48/2) = 48 + 24 = 72 #### 72\n",
      "****************************************************************************************************\n",
      "5\n",
      "Natalia sold clips to 48 friends in April, and half as many in May, so the equation is: 48 (April) + (48/2) (May) = 48 + 24 = ####72 ####72\n",
      "****************************************************************************************************\n",
      "6\n",
      "Natalia sold clips to 48 friends in April, and then sold half as many in May, so the equation representing the total number of clips sold in April and May is: 48 (April sales) + (1/2)*48 (May sales)\n",
      "\n",
      "Solving the equation: 48 + (1/2)*48 = 48 + 24 = 72\n",
      "\n",
      "#### 72\n",
      "****************************************************************************************************\n",
      "7\n",
      "Natalia sold clips to 48 friends in April and half as many in May, so the equation representing the total number of clips sold in April and May is:\n",
      "\n",
      "48 + (48/2) = 48 + 24\n",
      "\n",
      "#### 72\n",
      "****************************************************************************************************\n",
      "8\n",
      "Natalia sold clips to 48 friends in April, and half as many in May, so the equation is: 48 (April sales) + (48/2) (May sales) = 48 + 24 = 72 #### 72\n",
      "****************************************************************************************************\n",
      "9\n",
      "Natalia sold clips to 48 friends in April, and half as many in May, so the equation for May's sales is 48/2. To find the total number of clips sold in April and May, we add the sales of April (48) to the sales of May (48/2):\n",
      "\n",
      "48 + 48/2 = 48 + 24 = 72\n",
      "\n",
      "Thus, Natalia sold 72 clips altogether in April and May. #### 72\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(unique_correct_completions)):\n",
    "    print(i)\n",
    "    print(unique_correct_completions[i])\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that the best completion selected is indeed better in terms of the structure and reasoning steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-2-End generation based on trainset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7473 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "#!python generation.py --run-name \"corrected-pred-parser-1.0lev-\" --generation_path \"generations/gsm8k_synthetic_data_747instances_5samples_generations\" --beta_1 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "python generation.py --generation-path \"generations/gsm8k_synthetic_data_747instances_5samples_generations\" --run-name \"corrected-predp-parser-0.5levelwithtext-0.5leneq-\" --beta-1 0.5 --include-text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1.0 Lev - Mean Optimality Gap = 12.66\n",
    "- 0.75 Lev - Mean Optimality Gap = 1.13\n",
    "- 0.5 Lev - Mean Optimality Gap - 1.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eynvcw7sdau5"
   },
   "source": [
    "# Training\n",
    "***\n",
    "\n",
    "Employ whatever trick you would like to reduce the VRAM requirements during training (including swapping the model for a smaller one, although please only as a last resort)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlAK9jEqkqEm"
   },
   "outputs": [],
   "source": [
    "#!accelerate launch lora.py --train --run-name \"[attn-ffn]-lora-64r-64alpha-componly\" --no-evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%wand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tl8nfP8Hdsxf"
   },
   "source": [
    "# Evaluating the Model\n",
    "*** \n",
    "\n",
    "This final part is more free-form. We'd like to evaluate our new model on the test set to see if it's improved, but then spend however much time you have left examining the model more closely / demonstrating some interesting behaviour / showing off beautiful plots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you use align your accelerate config with ```deepspeed_inference.yaml``` provided before trying those scripts :-D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Phi-3.5 \n",
    "\n",
    "```bash\n",
    "accelerate launch lora.py --evaluate --model-name \"[attn-ffn]-lora-64r-64alpha_rft_747instances\" --no-train\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Experiment:baseline\n",
      "### Mean Maj@1 0.8257575757575758\n"
     ]
    }
   ],
   "source": [
    "paths_to_compare = {\n",
    "    \"baseline\": \"results/results_microsoft-Phi-3.5-mini-instruct_.csv\",\n",
    "}\n",
    "\n",
    "datas = {k : pd.read_csv(v) for k, v  in paths_to_compare.items()}\n",
    "\n",
    "for k, v in datas.items(): \n",
    "    mean_ = v[\"maj_1s\"].mean()\n",
    "    print(f\"### Experiment:{k}\\n### Mean Maj@1 {mean_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate LoRA Adaptation on the Union between two datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1. Completion utility is purely based on the Levenshiten Distance \n",
    "#### a.k.a $\\beta_1=1.0$\n",
    "\n",
    "[attn-ffn]-lora-64r-64alpha_rft_747instances \n",
    "\n",
    "stands for:\n",
    "\n",
    "Attention and feedforward attached LoRa using Rank=64 with Alpha=64 (stablised), trained on 747 Rejection-Sampled Instances \n",
    "\n",
    "\n",
    "cli: \n",
    "```bash\n",
    "accelerate launch lora.py --evaluate --model-name \"[attn-ffn]-lora-64r-64alpha_rft_747instances\" --no-train\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Experiment:baseline\n",
      "### Mean Maj@1 0.8257575757575758\n",
      "### Experiment:lora_adapted\n",
      "### Mean Maj@1 0.8005050505050505\n"
     ]
    }
   ],
   "source": [
    "paths_to_compare = {\n",
    "    \"baseline\": \"results/results_microsoft-Phi-3.5-mini-instruct_.csv\",\n",
    "    \"lora_adapted\": \"results/results_models-[attn-ffn]-lora-32r-25alpha-componly-uniondata-qlora_rft_747instances-checkpoint-117_.csv\",\n",
    "}\n",
    "\n",
    "datas = {k : pd.read_csv(v) for k, v  in paths_to_compare.items()}\n",
    "\n",
    "for k, v in datas.items(): \n",
    "    mean_ = v[\"maj_1s\"].mean()\n",
    "    print(f\"### Experiment:{k}\\n### Mean Maj@1 {mean_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate LoRA adapation purely based on generated data \n",
    "\n",
    "```bash\n",
    "accelerate launch lora.py --no-train --model-name \"[attn-ffn]-lora-16r-12alpha-componly-rftdataonly-qlora_rft_747instances/checkpoint-11\" --evaluate\n",
    "``` \n",
    "\n",
    "Evaluation stroed at ```results/results_[attn-ffn]-lora-16r-12alpha-componly-rftdataonly-qlora_rft_747instances-checkpoint-11_.csv``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly different LoRA config used in this experiment since it's a much smaller dataset to learn from: \n",
    "\n",
    "```python \n",
    "@dataclass\n",
    "class LoraLayersConfig:\n",
    "    \n",
    "    task_type: TaskType = TaskType.CAUSAL_LM\n",
    "    \n",
    "    r: int = 16\n",
    "    \n",
    "    lora_alpha: int = int(r*0.8)\n",
    "    \n",
    "    use_rslora:bool = True \n",
    "    \n",
    "    lora_dropout: float = 0.1\n",
    "    \n",
    "    bias: str = \"none\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Dataset:baseline\n",
      "### Mean Maj@1 0.8257575757575758\n",
      "### Dataset:lora_adapted\n",
      "### Mean Maj@1 0.8392255892255892\n"
     ]
    }
   ],
   "source": [
    "paths_to_compare = {\n",
    "    \"baseline\": \"results/results_microsoft-Phi-3.5-mini-instruct_.csv\",\n",
    "    \"lora_adapted\": \"results/results_[attn-ffn]-lora-16r-12alpha-componly-rftdataonly-qlora_rft_747instances-checkpoint-11_.csv\",\n",
    "    \n",
    "}\n",
    "\n",
    "datas = {k : pd.read_csv(v) for k, v  in paths_to_compare.items()}\n",
    "\n",
    "for k, v in datas.items(): \n",
    "    mean_ = v[\"maj_1s\"].mean()\n",
    "    print(f\"### Dataset:{k}\\n### Mean Maj@1 {mean_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash \n",
    "models/[beta1(0.5)-equation-with-text]-[attn-ffn]-[16r-12alpha]-[lr1e-03-gradacum1]-[componly]-[rftdataonly]-qlora_rft_747instances/checkpoint-22\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model trained through a weighted manner \n",
    "\n",
    "\n",
    "```bash \n",
    "accelerate launch lora.py --no-train --evaluate --model-name \"[attn-ffn]-lora-32r-25alpha-componly-uniondata-weighted-equationwithoutext-qlora_rft_747instances/checkpoint-117/\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7vm1ZTHw8E1"
   },
   "source": [
    "### [Optional] - Discussion\n",
    "\n",
    "We would be interested to know:\n",
    "\n",
    "1.   If you were less time / computationally constrained, what would you do differently?\n",
    "2.   What would your ideal first project look like if you joined?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXIw-pCiwGRk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
