{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_jcCK5ncrsw"
   },
   "source": [
    "# Convergence ML Engineer / Researcher Take-home\n",
    "\n",
    "We'd like to learn a little more about how you practically approach a small research-like project loosely based on Rejection Sampling Fine-tuning (aka RFT, introduced in https://arxiv.org/abs/2308.01825).\n",
    "\n",
    "Tip: focus on section 3.3 (\"Rejection Sampling Fine-tuning\"). The paper isn't the best written, and we're happy to clarify anything.\n",
    "\n",
    "We will provide some skeleton code for you to guide what we would like to see from you, although if you have ideas for a different structure you feel is better or more elegant, then feel free to rewrite and replace at will.\n",
    "\n",
    "Note: your final submission does not have to be in a colab notebook, does not have to use Hugging Face, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## A note from the team\n",
    "\n",
    "We want to give you a chance to show off some of your best abilities.\n",
    "\n",
    "For some people that might mean generating high quality data in a smart way. For others, it might be speeding up the whole process to enable easy reproducibility, and maybe organizing the code in a better way than given. Yet for others, it might be a chance to show off some modern policy optimization techniques like DPO or its variants. Or maybe focusing on solid evaluations and identifying limitations of small models and limited fine-tuning.\n",
    "\n",
    "An ideal outcome of course is some sense of the model improving its mathematical abilities, but itâ€™s not a bad thing if the final evaluation somehow shows equal or worse performance ðŸ˜‚ (negative results are results).\n",
    "\n",
    "Ask lots of question! We're happy to answer any questions about the assignment, and to discuss concepts like RFT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojrmHoW2dLdU"
   },
   "source": [
    "# Setup [ignore - just run]\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "id": "zPfAkbvHdEwJ"
   },
   "outputs": [],
   "source": [
    "#!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericliu/research/rft-gsm8k/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "import torch\n",
    "import random \n",
    "import datasets\n",
    "import numpy as np \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from data import GSM8KDataset\n",
    "import utils\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "SEED = 128 \n",
    "MODEL_NAME = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "# set seeds\n",
    "torch.random.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#Â accelerate inf \u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflash_attention_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_NAME)\n\u001b[1;32m     10\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mload_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgsm8k\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/research/rft-gsm8k/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:559\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    558\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m add_generation_mixin_to_remote_model(model_class)\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/research/rft-gsm8k/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3577\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3573\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3574\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3575\u001b[0m         )\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 3577\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   3578\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3579\u001b[0m         )\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[0;31mImportError\u001b[0m: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16, #Â accelerate inf \n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_dataset = datasets.load_dataset('gsm8k', 'main')['train']\n",
    "val_dataset = datasets.load_dataset('gsm8k', 'main')['test'] \n",
    "print(f\"Num Training instances: {len(train_dataset)}\")\n",
    "print(f\"Num Validation instances: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Inspection \n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    seed = np.random.randint(0, len(train_dataset))\n",
    "    print(\"*\"*100)\n",
    "    print(f\"Checking instance {seed}:\")\n",
    "    utils.inspect_instance(train_dataset, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only look at train set now for certain information that will be used\n",
    "during inference \n",
    "\n",
    "- Maximum length (num_tokens) of question: 239\n",
    "- Maximum length (num_tokens) of answer: 475 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    lambda x: utils.GSM8KParser.get_question_length(x['question'], tokenizer)\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x: utils.GSM8KParser.get_answer_length(x['answer'], tokenizer) \n",
    ")\n",
    "print(f\"Maximum answer num_tokens: {max(train_dataset['answer_length'])}\")\n",
    "print(f\"Maximum question num_tokens: {max(train_dataset['question_length'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â infer number of hops \n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x: utils.GSM8KParser.get_num_hops(x['answer'])\n",
    ")\n",
    "\n",
    "# infer answes using ground truth parser \n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x: utils.GSM8KParser.get_answer_from_gt(x['answer'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optinal Cell (Only to verify that parsing from \n",
    "# ground truth and parsing from completion would \n",
    "# yield the same result \n",
    "# infer answers using prediction parser\n",
    "answer_str_inf = [\n",
    "    utils.GSM8KParser.get_answer_from_pred(x)['answer_str_digit'] \\\n",
    "    for x in train_dataset['answer']\n",
    "]\n",
    "assert answer_str_inf == train_dataset['answer_str_digit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Vibe Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHIJ-OdodUW2"
   },
   "source": [
    "# Generating Synthetic Data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6J-3I8LKd_bM"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3.5-mini-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "\n",
    "ds = datasets.load_dataset(\"openai/gsm8k\", \"main\")\n",
    "\n",
    "def generate_synthetic_data(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    dataset: datasets.Dataset,\n",
    ") -> datasets.Dataset:\n",
    "    questions = dataset['question']\n",
    "\n",
    "    # Replace the next couple of lines with your own code to generate both a\n",
    "    # correct solution and an incorrect example from the model. You should\n",
    "    # generate 10 examples for each question and pick (one of) the best\n",
    "    # and worst respectively.\n",
    "\n",
    "    solutions = ['' for q in questions]\n",
    "    incorrect_examples = ['' for q in questions]\n",
    "\n",
    "    return datasets.Dataset.from_dict(\n",
    "      {\n",
    "          \"question\": questions,\n",
    "          \"solution\": solutions,\n",
    "          \"incorrect_example\": incorrect_examples,\n",
    "      }\n",
    "    )\n",
    "\n",
    "synthetic_dataset = generate_synthetic_data(model, tokenizer, ds['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eynvcw7sdau5"
   },
   "source": [
    "### Training on the Collected Data\n",
    "\n",
    "Of course, the computational resources of colab are limited.\n",
    "\n",
    "Employ whatever trick you would like to reduce the VRAM requirements during training (including swapping the model for a smaller one, although please only as a last resort)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlAK9jEqkqEm"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    lr: float = 3e-5\n",
    "    epochs: int = 2\n",
    "    batch_size: int = 4\n",
    "    device: str = 'cpu'\n",
    "\n",
    "def train(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    dataset: datasets.Dataset,\n",
    "    config: TrainConfig,\n",
    ") -> AutoModelForCausalLM:\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        # Implement this\n",
    "        return\n",
    "\n",
    "    def loss_fn(batch):\n",
    "        # Implement an appropriate loss - note we don't expect this to necessarily\n",
    "        # be tied to the earlier mentioned paper, just something that is sensible\n",
    "        return\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=config.lr)\n",
    "\n",
    "    dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    # This is a pretty bare-bones loop, feel free to add anything particularly useful\n",
    "    for epoch in config.epochs:\n",
    "        model.train()\n",
    "\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = loss_fn(**batch)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "amazing_model = train(model, tokenizer, synthetic_dataset, TrainConfig())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tl8nfP8Hdsxf"
   },
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "This final part is more free-form. We'd like to evaluate our new model on the test set to see if it's improved, but then spend however much time you have left examining the model more closely / demonstrating some interesting behaviour / showing off beautiful plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHZRLUzCuYCg"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: AutoModelForCausalLM,\n",
    "    eval_dataset: datasets.Dataset,\n",
    ") -> float:\n",
    "    return 0.0\n",
    "\n",
    "our_score = evaluate_model(amazing_model, ds['test'])\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3.5-mini-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "their_score = evaluate_model(original_model, ds['test'])\n",
    "\n",
    "conclusion = 'ðŸŽ‰ðŸŽ‰ðŸŽ‰' if our_score > their_score else 'oh well, was it even supposed to work?'\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7vm1ZTHw8E1"
   },
   "source": [
    "### [Optional] - Discussion\n",
    "\n",
    "We would be interested to know:\n",
    "\n",
    "1.   If you were less time / computationally constrained, what would you do differently?\n",
    "2.   What would your ideal first project look like if you joined?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXIw-pCiwGRk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
