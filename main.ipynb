{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFT - GMS8K \n",
    "- Candidate: Eric Liu "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os  \n",
    "import torch \n",
    "import numpy as np \n",
    "\n",
    "from tqdm import tqdm \n",
    "from textwrap import dedent  \n",
    "\n",
    "import utils \n",
    "import prompt \n",
    "from utils import GSM8KParser, GMS8KEvaluator\n",
    "from datasets import load_dataset\n",
    "from main import GSM8KDataset, Phi3LightningModule \n",
    "\n",
    "from sympy.parsing.sympy_parser import parse_expr \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pytorch_lightning as pl \n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import RichProgressBar \n",
    "\n",
    "import wandb \n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "MODEL_NAME = \"microsoft/Phi-3.5-mini-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 'pad_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.special_tokens_map_extended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instpect text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Training instances: 7473\n",
      "Num Validation instances: 1319\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('gsm8k', 'main')['train']\n",
    "val_dataset = load_dataset('gsm8k', 'main')['test'] \n",
    "print(f\"Num Training instances: {len(train_dataset)}\")\n",
    "print(f\"Num Validation instances: {len(val_dataset)}\")\n",
    "print(type(train_dataset)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 7473\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Checking instance 3917:\n",
      "question\n",
      "When Jack traveled to Canada, he had to wait 20 hours to get through customs, plus 14 days in coronavirus quarantine. How many hours total did Jack have to wait?\n",
      "answer\n",
      "First convert the quarantine wait time to hours by multiplying the number of days by the number of hours in a day: 14 days * 24 hours/day = <<14*24=336>>336 hours\n",
      "Then add the time Jack spent waiting in customs to the quarantine time: 336 hours + 20 hours = <<336+20=356>>356 hours\n",
      "#### 356\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Checking instance 3259:\n",
      "question\n",
      "Michael has $42. Michael gives away half the money to his brother. His brother then buys 3 dollars worth of candy. If his brother has $35 left, how much money, in dollars, did his brother have at first?\n",
      "answer\n",
      "Michael gives away 42/2=<<42/2=21>>21 dollars.\n",
      "Before buying candy, his brother has 35+3=<<35+3=38>>38 dollars.\n",
      "His brother had 38-21=<<38-21=17>>17 dollars at first.\n",
      "#### 17\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Checking instance 3403:\n",
      "question\n",
      "Diane is twice as old as her brother, Will. If Will was 4 years old 3 years ago, what will the sum of their ages be in 5 years?\n",
      "answer\n",
      "Since Will was 4 years old 3 years ago, then he is 4+3 = <<4+3=7>>7 years old now\n",
      "Dina is twice as old as he is so she is 2*7 = <<2*7=14>>14 years old now\n",
      "In 5 years, Will will be 7+5 = <<7+5=12>>12 years old\n",
      "In 5 years, Diane will be 14+5 = 19 years old\n",
      "The sum of their ages at that time will be 12+19 = <<12+19=31>>31 years\n",
      "#### 31\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Checking instance 6101:\n",
      "question\n",
      "Cary is saving money to buy a new pair of shoes that cost $120. He has already saved $30. He earns $5 for every lawn he mows. If he mows 3 lawns each weekend, how many more weekends will he have to mow lawns before he can afford to buy the shoes?\n",
      "answer\n",
      "Cary needs to save an additional $120 - $30 = $<<120-30=90>>90.\n",
      "Cary makes $5 * 3 = $<<5*3=15>>15 each weekend.\n",
      "It will take 90 / 15 = <<90/15=6>>6 more weekends to save enough money to afford the shoes.\n",
      "#### 6\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Checking instance 4617:\n",
      "question\n",
      "Maggie has an after-school job that pays her $5.00 for every magazine subscription she can sell.  She sells 4 to her parents, 1 to her grandfather, 2 to the next-door neighbor and twice that amount to another neighbor.  How much money did Maggie earn?\n",
      "answer\n",
      "Maggie sells 2 subscriptions to a neighbor and twice that amount to another neighbor so she sells 2*2 = <<2*2=4>>4 subscriptions to the other neighbor\n",
      "In total, Maggie sells 4+1+2+4 = <<4+1+2+4=11>>11 subscriptions\n",
      "She earns $5.00 per subscription she sells so she earns 5*11 = $<<5*11=55.00>>55.00\n",
      "#### 55\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    seed = np.random.randint(0, len(train_dataset))\n",
    "    print(\"*\"*100)\n",
    "    print(f\"Checking instance {seed}:\")\n",
    "    utils.inspect_instance(train_dataset, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Num Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only look at train set now for certain information that will be used\n",
    "during inference \n",
    "\n",
    "- Maximum length (num_tokens) of question: 239\n",
    "- Maximum length (num_tokens) of answer: 475 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum answer num_tokens: 475\n",
      "Maximum question num_tokens: 239\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    lambda x: GSM8KParser.get_question_length(x['question'], tokenizer)\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x: GSM8KParser.get_answer_length(x['answer'], tokenizer) \n",
    ")\n",
    "print(f\"Maximum answer num_tokens: {max(train_dataset['answer_length'])}\")\n",
    "print(f\"Maximum question num_tokens: {max(train_dataset['question_length'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â infer number of hops \n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x: GSM8KParser.get_num_hops(x['answer'])\n",
    ")\n",
    "\n",
    "# infer answes using ground truth parser \n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x: GSM8KParser.get_answer_from_gt(x['answer'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optinal Cell (Only to verify that parsing from \n",
    "# ground truth and parsing from completion would \n",
    "# yield the same result \n",
    "\n",
    "# infer answers using prediction parser\n",
    "answer_str_inf = [\n",
    "    GSM8KParser.get_answer_from_pred(x)['answer_str_digit'] \\\n",
    "    for x in train_dataset['answer']\n",
    "]\n",
    "assert answer_str_inf == train_dataset['answer_str_digit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance Generation \n",
    "We selected the longest dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    \"max_new_tokens\" : MAX_NEW_TOKNES_SAMPE,\n",
    "    \"temperature\": 0.7,\n",
    "    \"num_return_sequences\":1,\n",
    "    \"top_p\": 0.9,\n",
    "    \"eos_token_id\":tokenizer.eos_token_id,  # Specify the EOS token\n",
    "    \"pad_token_id\":tokenizer.eos_token_id, \n",
    "    \"do_sample\":True,\n",
    "    \"output_scores\":True,\n",
    "    \"return_dict_in_generate\":True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = sorted(\n",
    "    train_dataset, \n",
    "    key=lambda x: x['answer_length'], \n",
    "    reverse=True\n",
    ")[50]\n",
    "\n",
    "chat = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": prompt.EvalTemplate.system\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt.EvalTemplate.user.format(\n",
    "            question=instance['question'],\n",
    "            eos_token = tokenizer.eos_token,\n",
    "        )\n",
    "    }\n",
    "]\n",
    "prompts = tokenizer.apply_chat_template(\n",
    "    [chat],  \n",
    "    add_generation_prompt=True,\n",
    "    tokenize = False,\n",
    "    return_tensors='pt',\n",
    "    )\n",
    "\n",
    "print(len(prompts))\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(instance[\"answer\"], instance[\"answer_str_digit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "outs = utils.sample_answers(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    prompts,\n",
    "    **generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [GSM8KParser.get_answer_from_pred(out)[\"answer_str_digit\"] for out in outs]\n",
    "print(preds)\n",
    "\n",
    "evaluator = GMS8KEvaluator()\n",
    "refs =  [instance[\"answer_str_digit\"]]\n",
    "print(refs)\n",
    "\n",
    "maj_accs = [\n",
    "    evaluator.get_maj_at_k(pred, ref) \\\n",
    "    for pred, ref in zip(preds, refs)\n",
    "]\n",
    "\n",
    "print(maj_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    GSM8KParser.get_answer_from_pred(out[1])[\"answer_str_digit\"]\n",
    ")\n",
    "\n",
    "print(instance[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = GMS8KEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model Eval \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before we start, let's get a good hang of the performance of the base model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum answer num_tokens: 430\n",
      "Maximum question num_tokens: 289\n",
      "Maximum sequence num_tokens: 719\n",
      "Maximum new tokens in generation: 1024\n",
      "Setup Completed dataset:\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'answer_str_digit', 'question_length', 'answer_length', 'question_input_ids', 'question_attention_mask', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1319\n",
      "})\n",
      "Maximum num_tokens for inference: 1024\n"
     ]
    }
   ],
   "source": [
    "valData = GSM8KDataset(val_dataset, tokenizer)\n",
    "val_dataloader = DataLoader(\n",
    "    valData, \n",
    "    batch_size=4, \n",
    "    shuffle=False, \n",
    "    num_workers=16,\n",
    ")\n",
    "generation_config = {\n",
    "    \"max_new_tokens\" : valData.inf_seq_length,\n",
    "    \"temperature\": 0.7,\n",
    "    \"num_return_sequences\":1,\n",
    "    \"top_p\": 0.9,\n",
    "    \"eos_token_id\":tokenizer.eos_token_id,  # Specify the EOS token\n",
    "    \"pad_token_id\":tokenizer.eos_token_id, \n",
    "    \"do_sample\":True,\n",
    "    \"output_scores\":True,\n",
    "    \"return_dict_in_generate\":True,\n",
    "    #\"cache_implementation\":\"static\"\n",
    "}#     val_loader = DataLoader(val_data, batch_size=4, num_workers=4)\n",
    "print(f\"Maximum num_tokens for inference: {valData.inf_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c423a1ef35a245c0a8815aa96a3754f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "module = Phi3LightningModule(\n",
    "    MODEL_NAME, \n",
    "    generation_config=generation_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "    project=\"phi3-gsm8k-training\", \n",
    "    log_model=\"all\"\n",
    ")\n",
    "\n",
    "pbar = RichProgressBar()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"auto\",\n",
    "    devices=-1,\n",
    "    logger=wandb_logger,\n",
    "    #strategy='DDP',\n",
    "    #callbacks=[RichProgressBar()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA L40S') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdafd86ef52d4189bf5b3acfd44951dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "The current flash attention version does not support sliding window attention. Please use `attn_implementation='eager'` or upgrade flash-attn library.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.test(\n",
    "    module,  \n",
    "    dataloaders=val_dataloader, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rejection Sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0 \n",
    "instance = sorted_data[idx]\n",
    "\n",
    "chat = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": prompt.Template.system\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt.Template.user.format(question=instance['question'])\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idxs = [100, 1000, 2000]\n",
    "\n",
    "# convs = []\n",
    "# for idx in idxs:\n",
    "#     instance = train_dataset[idx] \n",
    "#     conv =[\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": prompt.Template.system\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": prompt.Template.user.format(question=instance['question'])\n",
    "#         }\n",
    "#     ]\n",
    "#     convs.append(conv)\n",
    "\n",
    "chats = tokenizer.apply_chat_template(\n",
    "    [chat],  \n",
    "    add_generation_prompt=True,\n",
    "    tokenize = False,\n",
    "    return_tensors='pt',\n",
    "    )\n",
    "print(type(chat), len(chats))\n",
    "print(chats[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.batch_encode_plus(chats, return_tensors='pt', padding='longest')[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import is_flash_attn_2_available \n",
    "is_flash_attn_2_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "samples = utils.sample_answers(\n",
    "    tokenizer=tokenizer, \n",
    "    model=model, \n",
    "    chats = chats,\n",
    "    max_new_tokens=256, \n",
    "    temperature=0.5,\n",
    "    num_samples=10,\n",
    "    top_p= 0.85,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_samples_base = ''\n",
    "\n",
    "for sample in samples:\n",
    "    rand_samples_base += (sample + '\\n') \n",
    "    rand_samples_base += (\"*\"*50 + '\\n') \n",
    "\n",
    "print(rand_samples_base)\n",
    "with open(\"long_hop.txt\", 'w') as f:\n",
    "    f.write(rand_samples_base)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
